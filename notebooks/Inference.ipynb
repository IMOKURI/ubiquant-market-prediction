{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def in_kaggle():\n",
    "    return \"kaggle_web_client\" in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "def init_logger():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler1 = logging.StreamHandler(stream=sys.stdout)\n",
    "    handler1.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] [%(module)s] %(message)s\"))\n",
    "    # handler2 = logging.FileHandler(filename=\"train.log\")\n",
    "    # handler2.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] [%(module)s] %(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    # logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "log = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INSERT SOURCE CODE HERE FOR SUBMISSION #####\n",
    "\n",
    "if in_kaggle():\n",
    "    sys.path.append(\"../input/nptyping\")\n",
    "    sys.path.append(\"../input/typish\")\n",
    "else:\n",
    "    sys.path.append(\"..\")\n",
    "    sys.path.append(\"../../inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_kaggle():\n",
    "    # https://www.kaggle.com/speeddemon/install-hydra-offline-from-dataset\n",
    "\n",
    "    !cp -r /kaggle/input/hydracore105 /kaggle/working\n",
    "    !mv /kaggle/working/hydracore105/antlr4-python3-runtime-4.8.tar.gz.tmp /kaggle/working/hydracore105/antlr4-python3-runtime-4.8.tar.gz\n",
    "    !ls /kaggle/working/hydracore105\n",
    "\n",
    "    !pip install -qq /kaggle/working/hydracore105/* --ignore-installed PyYAML\n",
    "    \n",
    "    sys.path.append(\"../input/omegaconf/omegaconf-master\")\n",
    "    omega_conf_path = \"config/main.yaml\"\n",
    "    \n",
    "else:\n",
    "    omega_conf_path = \"../config/main.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "c = OmegaConf.load(omega_conf_path)\n",
    "\n",
    "c.settings.debug = False\n",
    "c.wandb.enabled = False\n",
    "\n",
    "if in_kaggle():\n",
    "    c.settings.gpus = \"0\"\n",
    "\n",
    "    c.settings.dirs.working = \".\"\n",
    "    c.settings.dirs.input = \"../input/ubiquant-market-prediction/\"\n",
    "    c.settings.dirs.feature = \"../input/ubiquant-parquet/\"\n",
    "\n",
    "    pretraind_dir = \"../input/ump-models\"\n",
    "\n",
    "else:\n",
    "    c.settings.dirs.working = \"..\"\n",
    "    c.settings.dirs.input = \"../../inputs/\"\n",
    "\n",
    "    pretraind_dir = \"../../datasets/trainings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = f\"\"\"\n",
    "- dir: {pretraind_dir}/2022-02-04_23-43-25/fold0/\n",
    "  model: ump_1\n",
    "\"\"\"\n",
    "\n",
    "pretrained_lgb = f\"\"\"\n",
    "- dir: {pretraind_dir}/2022-02-08_16-16-05/fold0/\n",
    "  model: lightgbm\n",
    "\"\"\"\n",
    "\n",
    "_pretrained = f\"\"\"\n",
    "- dir: {pretraind_dir}/2022-02-04_23-43-27/fold1/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-04_23-43-29/fold2/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-04_23-43-31/fold3/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-04_23-43-33/fold4/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-05_08-05-00/fold5/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-05_08-05-02/fold6/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-05_08-05-04/fold7/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-05_08-05-06/fold8/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-05_08-05-08/fold9/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-05_20-09-55/fold10/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-05_20-09-57/fold11/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-05_20-09-59/fold12/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-05_20-10-01/fold13/\n",
    "  model: ump_1\n",
    "- dir: {pretraind_dir}/2022-02-05_20-10-04/fold14/\n",
    "  model: ump_1\n",
    "\"\"\"\n",
    "\n",
    "c.params.pretrained = OmegaConf.create(pretrained)\n",
    "c.params.pretrained_lgb = OmegaConf.create(pretrained_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 20:40:20,149 [INFO] [3244290467] defaults:\n",
      "- _self_\n",
      "hydra:\n",
      "  run:\n",
      "    dir: ../outputs/${now:%Y-%m-%d_%H-%M-%S}\n",
      "  job_logging:\n",
      "    formatters:\n",
      "      simple:\n",
      "        format: '%(asctime)s [%(levelname)s][%(module)s] %(message)s'\n",
      "wandb:\n",
      "  enabled: false\n",
      "  entity: imokuri\n",
      "  project: ump\n",
      "  dir: ${hydra:runtime.cwd}/../cache\n",
      "  group: default\n",
      "settings:\n",
      "  print_freq: 100\n",
      "  gpus: 6,7\n",
      "  dirs:\n",
      "    working: ..\n",
      "    input: ../../inputs/\n",
      "    feature: ${settings.dirs.input}features/\n",
      "    preprocess: ${settings.dirs.input}preprocess/\n",
      "  inputs:\n",
      "  - train.csv\n",
      "  - example_test.csv\n",
      "  - example_sample_submission.csv\n",
      "  debug: false\n",
      "  n_debug_data: 100000\n",
      "  amp: true\n",
      "  multi_gpu: true\n",
      "  training_method: nn\n",
      "params:\n",
      "  seed: 440\n",
      "  n_class: 1\n",
      "  preprocess: false\n",
      "  n_fold: 5\n",
      "  skip_training: false\n",
      "  epoch: 20\n",
      "  es_patience: 0\n",
      "  batch_size: 640\n",
      "  gradient_acc_step: 1\n",
      "  max_grad_norm: 1000\n",
      "  fold: simple_cpcv\n",
      "  group_name: investment_id\n",
      "  time_name: time_id\n",
      "  label_name: target\n",
      "  use_feature: true\n",
      "  feature_set:\n",
      "  - f000\n",
      "  dataset: ump_1\n",
      "  model: ump_1\n",
      "  pretrained:\n",
      "  - dir: ../../datasets/trainings/2022-02-04_23-43-25/fold0/\n",
      "    model: ump_1\n",
      "  criterion: RMSELoss\n",
      "  optimizer: Adam\n",
      "  scheduler: CosineAnnealingWarmupRestarts\n",
      "  lr: 0.001\n",
      "  min_lr: 1.0e-06\n",
      "  weight_decay: 1.0e-05\n",
      "  label_smoothing: 1.0e-06\n",
      "  scoring: pearson\n",
      "  pretrained_lgb:\n",
      "  - dir: ../../datasets/trainings/2022-02-08_16-16-05/fold0/\n",
      "    model: lightgbm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.info(OmegaConf.to_yaml(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import src.utils as utils\n",
    "import ubiquant\n",
    "from src.feature_store import Store\n",
    "from src.features.base import get_feature\n",
    "from src.make_feature import make_feature\n",
    "from src.make_model import load_model\n",
    "from src.run_loop import inference, inference_lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_seed(c.params.seed)\n",
    "utils.debug_settings(c)\n",
    "device = utils.gpu_settings(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 20:40:21,825 [INFO] [2937476863] feature set: ['f000']\n"
     ]
    }
   ],
   "source": [
    "feature_set = [\"f000\"]\n",
    "\n",
    "feature_set = list(sorted(list(set(feature_set))))\n",
    "log.info(f\"feature set: {feature_set}\")\n",
    "\n",
    "feature_func = [get_feature(f) for f in feature_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = Store.empty()\n",
    "# store = Store.train(c)  # Your notebook tried to allocate more memory than is available.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_model(c, device)\n",
    "models_lgb = load_model(c, device, c.params.pretrained_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ubiquant.make_env()  # initialize the environment\n",
    "iter_test = env.iter_test()  # an iterator which loops over the test set and sample submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [f\"f_{n}\" for n in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    }
   ],
   "source": [
    "for test_df, sample_prediction_df in iter_test:\n",
    "    gc.collect()\n",
    "\n",
    "    # log.info(test_df[\"investment_id\"].dtype)  # int16\n",
    "    # log.info(test_df[feature_cols].values.dtype)  # float64\n",
    "\n",
    "    try:\n",
    "        assert len(test_df[\"investment_id\"].unique()) == len(test_df[\"investment_id\"]), \"investment_id is not unique.\"\n",
    "\n",
    "        if c.params.use_feature:\n",
    "            for row in test_df.values:\n",
    "                # investment_id_ = int(row[1])\n",
    "                # features_ = row[2:302].astype(np.float32)\n",
    "\n",
    "                # log.info(f\"investment_id: {investment_id_}({type(investment_id_)}), features: {len(features_)}({features_.dtype})\")\n",
    "                store.append(row)\n",
    "\n",
    "                # log.info(f\"store: {store.investments[investment_id_].features.last_n(1).squeeze().shape}, input: {features_.shape}\")\n",
    "                # assert np.array_equal(\n",
    "                #     store.investments[investment_id_].features.last_n(1).squeeze(), features_\n",
    "                # ), \"Features are different before and after storing in the store\"\n",
    "\n",
    "            pred_df = make_feature(\n",
    "                test_df,\n",
    "                store,\n",
    "                feature_set,\n",
    "                load_from_store=False,\n",
    "                save_to_store=False,\n",
    "                debug=c.settings.debug,\n",
    "            )\n",
    "\n",
    "            assert len(test_df) == len(pred_df), \"test_df and pred_df do not same size.\"\n",
    "            assert list(pred_df.columns) == feature_cols, \"pred_df has feature_cols columns.\"\n",
    "\n",
    "            # assert (\n",
    "            #     test_df[feature_cols].astype(\"float32\").equals(pred_df)\n",
    "            # ), \"Default features do not match between test_df and pred_df.\"\n",
    "\n",
    "            preds = inference(c, pred_df, device, models)\n",
    "            preds_lgb = inference_lightgbm(pred_df, models_lgb)\n",
    "\n",
    "            # else:\n",
    "            preds_ = inference(c, test_df, device, models)\n",
    "            preds_lgb_ = inference_lightgbm(test_df, models_lgb)\n",
    "\n",
    "        # assert np.array_equal(preds_, preds), \"Predictions do not match between test_df and pred_df.\"\n",
    "        # assert np.array_equal(preds_lgb_, preds_lgb), \"LightGBM predictions do not match between test_df and pred_df.\"\n",
    "\n",
    "        predictions = np.hstack([preds_, preds_lgb_])\n",
    "        sample_prediction_df[\"target\"] = np.nanmean(predictions, axis=1)\n",
    "\n",
    "        # DEBUG\n",
    "        # sample_prediction_df.fillna({\"target\": 0}, inplace=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        log.warning(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    env.predict(sample_prediction_df)  # register your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-all-in-one",
   "language": "python",
   "name": "py37-all-in-one"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
