{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def in_kaggle():\n",
    "    return \"kaggle_web_client\" in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "def init_logger():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler1 = logging.StreamHandler(stream=sys.stdout)\n",
    "    handler1.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] [%(module)s] %(message)s\"))\n",
    "    # handler2 = logging.FileHandler(filename=\"train.log\")\n",
    "    # handler2.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] [%(module)s] %(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    # logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "log = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INSERT SOURCE CODE HERE FOR SUBMISSION #####\n",
    "\n",
    "if in_kaggle():\n",
    "    sys.path.append(\"../input/nptyping\")\n",
    "    sys.path.append(\"../input/typish\")\n",
    "else:\n",
    "    sys.path.append(\"..\")\n",
    "    sys.path.append(\"../../inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_kaggle():\n",
    "    !pip install -qq --no-index --find-links ../input/faissgpu17 -r ../input/faissgpu17/requirements.txt\n",
    "\n",
    "    !pip install -qq ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n",
    "    \n",
    "    # https://www.kaggle.com/speeddemon/install-hydra-offline-from-dataset\n",
    "    !cp -r /kaggle/input/hydracore105 /kaggle/working\n",
    "    !mv /kaggle/working/hydracore105/antlr4-python3-runtime-4.8.tar.gz.tmp /kaggle/working/hydracore105/antlr4-python3-runtime-4.8.tar.gz\n",
    "    !ls /kaggle/working/hydracore105\n",
    "\n",
    "    !pip install -qq /kaggle/working/hydracore105/* --ignore-installed PyYAML\n",
    "    \n",
    "    sys.path.append(\"../input/omegaconf/omegaconf-master\")\n",
    "    omega_conf_path = \"config/main.yaml\"\n",
    "    \n",
    "else:\n",
    "    omega_conf_path = \"../config/main.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "c = OmegaConf.load(omega_conf_path)\n",
    "\n",
    "c.settings.debug = False\n",
    "c.wandb.enabled = False\n",
    "\n",
    "if in_kaggle():\n",
    "    c.settings.gpus = \"0\"\n",
    "\n",
    "    c.settings.dirs.working = \".\"\n",
    "    c.settings.dirs.input = \"../input/ubiquant-market-prediction/\"\n",
    "    c.settings.dirs.input_minimal = \"../input/ump-train-minimal/\"\n",
    "    c.settings.dirs.feature = \"../input/ubiquant-parquet/\"\n",
    "    c.settings.dirs.preprocess = \"../input/ump-preprocess/\"\n",
    "\n",
    "    pretraind_dir = \"../input/ump-models\"\n",
    "\n",
    "else:\n",
    "    c.settings.dirs.working = \"..\"\n",
    "    c.settings.dirs.input = \"../../inputs/\"\n",
    "    c.settings.dirs.input_minimal = \"../../datasets/inputs/\"\n",
    "    c.settings.dirs.preprocess = \"../../inputs/preprocess/\"\n",
    "\n",
    "    pretraind_dir = \"../../datasets/trainings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = f\"\"\"\n",
    "\n",
    "- dir: {pretraind_dir}/2022-03-11_20-25-09/fold1/\n",
    "  model: ump_1\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-11_20-25-15/fold2/\n",
    "  model: ump_1\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-11_20-26-13/fold4/\n",
    "  model: ump_1\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-11_20-26-19/fold5/\n",
    "  model: ump_1\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "\n",
    "- dir: {pretraind_dir}/2022-03-12_21-58-26/fold1/\n",
    "  model: ump_1dcnn\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-12_21-58-32/fold2/\n",
    "  model: ump_1dcnn\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-12_21-59-36/fold4/\n",
    "  model: ump_1dcnn\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-12_21-59-41/fold5/\n",
    "  model: ump_1dcnn\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pretrained_2 = f\"\"\"\n",
    "\n",
    "- dir: {pretraind_dir}/2022-03-17_20-47-04/fold1/\n",
    "  model: ump_lstm\n",
    "  model_input: 300\n",
    "  n_class: 10\n",
    "  feature_set: [\"f001\", \"f902\"]\n",
    "- dir: {pretraind_dir}/2022-03-17_20-47-09/fold2/\n",
    "  model: ump_lstm\n",
    "  model_input: 300\n",
    "  n_class: 10\n",
    "  feature_set: [\"f001\", \"f902\"]\n",
    "- dir: {pretraind_dir}/2022-03-17_20-47-41/fold4/\n",
    "  model: ump_lstm\n",
    "  model_input: 300\n",
    "  n_class: 10\n",
    "  feature_set: [\"f001\", \"f902\"]\n",
    "- dir: {pretraind_dir}/2022-03-17_20-47-47/fold5/\n",
    "  model: ump_lstm\n",
    "  model_input: 300\n",
    "  n_class: 10\n",
    "  feature_set: [\"f001\", \"f902\"]\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pretrained_lgb = f\"\"\"\n",
    "\n",
    "- dir: {pretraind_dir}/2022-03-13_20-45-32/fold1/ \n",
    "  model: lightgbm\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-13_20-45-32/fold2/ \n",
    "  model: lightgbm\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-13_20-45-32/fold4/ \n",
    "  model: lightgbm\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-13_20-45-32/fold5/ \n",
    "  model: lightgbm\n",
    "  n_class: 1\n",
    "\n",
    "- dir: {pretraind_dir}/2022-03-18_10-49-32/fold1/tabnet/ \n",
    "  model: tabnet\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-18_10-49-32/fold2/tabnet/ \n",
    "  model: tabnet\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-18_10-49-32/fold4/tabnet/ \n",
    "  model: tabnet\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-18_10-49-32/fold5/tabnet/ \n",
    "  model: tabnet\n",
    "  n_class: 1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "_pretrained = f\"\"\"\n",
    "\n",
    "- dir: {pretraind_dir}/2022-03-11_20-25-04/fold0/\n",
    "  model: ump_1\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-11_20-26-08/fold3/\n",
    "  model: ump_1\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "\n",
    "- dir: {pretraind_dir}/2022-03-12_21-58-21/fold0/\n",
    "  model: ump_1dcnn\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-12_21-59-31/fold3/\n",
    "  model: ump_1dcnn\n",
    "  model_input: 300\n",
    "  n_class: 1\n",
    "\n",
    "- dir: {pretraind_dir}/2022-03-17_20-46-59/fold0/\n",
    "  model: ump_lstm\n",
    "  model_input: 300\n",
    "  n_class: 10\n",
    "  feature_set: [\"f001\", \"f902\"]\n",
    "- dir: {pretraind_dir}/2022-03-17_20-47-36/fold3/\n",
    "  model: ump_lstm\n",
    "  model_input: 300\n",
    "  n_class: 10\n",
    "  feature_set: [\"f001\", \"f902\"]\n",
    "\n",
    "- dir: {pretraind_dir}/2022-03-13_20-45-32/fold0/ \n",
    "  model: lightgbm\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-13_20-45-32/fold3/ \n",
    "  model: lightgbm\n",
    "  n_class: 1\n",
    "\n",
    "- dir: {pretraind_dir}/2022-03-18_10-49-32/fold0/tabnet/ \n",
    "  model: tabnet\n",
    "  n_class: 1\n",
    "- dir: {pretraind_dir}/2022-03-18_10-49-32/fold3/tabnet/ \n",
    "  model: tabnet\n",
    "  n_class: 1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "c.params.pretrained = OmegaConf.create(pretrained)\n",
    "c.params.pretrained_2 = OmegaConf.create(pretrained_2)\n",
    "c.params.pretrained_lgb = OmegaConf.create(pretrained_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-18 20:01:04,095 [INFO] [3244290467] defaults:\n",
      "- _self_\n",
      "hydra:\n",
      "  run:\n",
      "    dir: ../outputs/${now:%Y-%m-%d_%H-%M-%S}\n",
      "  job_logging:\n",
      "    formatters:\n",
      "      simple:\n",
      "        format: '%(asctime)s [%(levelname)s][%(module)s] %(message)s'\n",
      "wandb:\n",
      "  enabled: false\n",
      "  entity: imokuri\n",
      "  project: ump\n",
      "  dir: ${hydra:runtime.cwd}/../cache\n",
      "  group: default\n",
      "settings:\n",
      "  print_freq: 100\n",
      "  gpus: '6'\n",
      "  dirs:\n",
      "    working: ..\n",
      "    input: ../../inputs/\n",
      "    input_minimal: ../../datasets/inputs/\n",
      "    feature: ${settings.dirs.input}features/\n",
      "    preprocess: ../../inputs/preprocess/\n",
      "  inputs:\n",
      "  - train.csv\n",
      "  - example_test.csv\n",
      "  - example_sample_submission.csv\n",
      "  debug: false\n",
      "  n_debug_data: 100000\n",
      "  amp: true\n",
      "  multi_gpu: true\n",
      "  training_method: nn\n",
      "params:\n",
      "  seed: 7440\n",
      "  n_class: 1\n",
      "  preprocess: []\n",
      "  pca_n_components: 50\n",
      "  n_fold: 5\n",
      "  skip_training: false\n",
      "  epoch: 10\n",
      "  es_patience: 0\n",
      "  batch_size: 640\n",
      "  gradient_acc_step: 1\n",
      "  max_grad_norm: 1000\n",
      "  fold: combinational_purged\n",
      "  group_name: investment_id\n",
      "  time_name: time_id\n",
      "  label_name: target\n",
      "  use_feature: true\n",
      "  feature_set:\n",
      "  - f000\n",
      "  dataset: ump_1\n",
      "  model: ump_1dcnn_small\n",
      "  model_input: 300\n",
      "  model_window: 10\n",
      "  pretrained:\n",
      "  - dir: ../../datasets/trainings/2022-03-11_20-25-09/fold1/\n",
      "    model: ump_1\n",
      "    model_input: 300\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-11_20-25-15/fold2/\n",
      "    model: ump_1\n",
      "    model_input: 300\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-11_20-26-13/fold4/\n",
      "    model: ump_1\n",
      "    model_input: 300\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-11_20-26-19/fold5/\n",
      "    model: ump_1\n",
      "    model_input: 300\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-12_21-58-26/fold1/\n",
      "    model: ump_1dcnn\n",
      "    model_input: 300\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-12_21-58-32/fold2/\n",
      "    model: ump_1dcnn\n",
      "    model_input: 300\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-12_21-59-36/fold4/\n",
      "    model: ump_1dcnn\n",
      "    model_input: 300\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-12_21-59-41/fold5/\n",
      "    model: ump_1dcnn\n",
      "    model_input: 300\n",
      "    n_class: 1\n",
      "  criterion: RMSELoss\n",
      "  optimizer: Adam\n",
      "  scheduler: CosineAnnealingWarmupRestarts\n",
      "  lr: 0.001\n",
      "  min_lr: 1.0e-06\n",
      "  weight_decay: 1.0e-05\n",
      "  label_smoothing: 1.0e-06\n",
      "  scoring: pearson\n",
      "  pretrained_2:\n",
      "  - dir: ../../datasets/trainings/2022-03-17_20-47-04/fold1/\n",
      "    model: ump_lstm\n",
      "    model_input: 300\n",
      "    n_class: 10\n",
      "    feature_set:\n",
      "    - f001\n",
      "    - f902\n",
      "  - dir: ../../datasets/trainings/2022-03-17_20-47-09/fold2/\n",
      "    model: ump_lstm\n",
      "    model_input: 300\n",
      "    n_class: 10\n",
      "    feature_set:\n",
      "    - f001\n",
      "    - f902\n",
      "  - dir: ../../datasets/trainings/2022-03-17_20-47-41/fold4/\n",
      "    model: ump_lstm\n",
      "    model_input: 300\n",
      "    n_class: 10\n",
      "    feature_set:\n",
      "    - f001\n",
      "    - f902\n",
      "  - dir: ../../datasets/trainings/2022-03-17_20-47-47/fold5/\n",
      "    model: ump_lstm\n",
      "    model_input: 300\n",
      "    n_class: 10\n",
      "    feature_set:\n",
      "    - f001\n",
      "    - f902\n",
      "  pretrained_lgb:\n",
      "  - dir: ../../datasets/trainings/2022-03-13_20-45-32/fold1/\n",
      "    model: lightgbm\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-13_20-45-32/fold2/\n",
      "    model: lightgbm\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-13_20-45-32/fold4/\n",
      "    model: lightgbm\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-13_20-45-32/fold5/\n",
      "    model: lightgbm\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-18_10-49-32/fold1/tabnet/\n",
      "    model: tabnet\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-18_10-49-32/fold2/tabnet/\n",
      "    model: tabnet\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-18_10-49-32/fold4/tabnet/\n",
      "    model: tabnet\n",
      "    n_class: 1\n",
      "  - dir: ../../datasets/trainings/2022-03-18_10-49-32/fold5/tabnet/\n",
      "    model: tabnet\n",
      "    n_class: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.info(OmegaConf.to_yaml(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import src.utils as utils\n",
    "import ubiquant\n",
    "from src.feature_store import Store\n",
    "from src.features.base import get_feature\n",
    "from src.make_feature import make_feature\n",
    "from src.make_model import load_model\n",
    "from src.run_loop import inference, inference_lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_seed(c.params.seed)\n",
    "utils.debug_settings(c)\n",
    "device = utils.gpu_settings(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-18 20:01:06,061 [INFO] [573233744] feature set: ['f000']\n",
      "2022-03-18 20:01:06,062 [INFO] [573233744] feature set 2: ['f001']\n"
     ]
    }
   ],
   "source": [
    "feature_set = [\"f000\"]\n",
    "feature_set = list(sorted(list(set(feature_set))))\n",
    "log.info(f\"feature set: {feature_set}\")\n",
    "\n",
    "feature_set_2 = [\"f001\"]\n",
    "feature_set_2 = list(sorted(list(set(feature_set_2))))\n",
    "log.info(f\"feature set 2: {feature_set_2}\")\n",
    "\n",
    "# feature_func = [get_feature(f) for f in feature_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if c.params.preprocess:\n",
    "    store = Store.train(c)\n",
    "else:\n",
    "    store = Store.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "models = load_model(c, device, c.params.pretrained)\n",
    "models_2 = load_model(c, device, c.params.pretrained_2)\n",
    "models_lgb = load_model(c, device, c.params.pretrained_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ubiquant.make_env()  # initialize the environment\n",
    "iter_test = env.iter_test()  # an iterator which loops over the test set and sample submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols = [f\"f_{n}\" for n in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "2022-03-18 20:01:14,211 [INFO] [<timed exec>] predictions size: (2, 20)\n",
      "2022-03-18 20:01:17,882 [INFO] [<timed exec>] predictions size: (3, 20)\n",
      "2022-03-18 20:01:21,493 [INFO] [<timed exec>] predictions size: (3, 20)\n",
      "2022-03-18 20:01:25,115 [INFO] [<timed exec>] predictions size: (1, 20)\n",
      "CPU times: user 32.3 s, sys: 8.96 s, total: 41.2 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for test_df, sample_prediction_df in iter_test:\n",
    "    gc.collect()\n",
    "\n",
    "    # log.info(test_df[\"investment_id\"].dtype)  # int16\n",
    "    # log.info(test_df[feature_cols].values.dtype)  # float64\n",
    "\n",
    "    predictions_ = []\n",
    "\n",
    "    try:\n",
    "        # assert len(test_df[\"investment_id\"].unique()) == len(test_df[\"investment_id\"]), \"investment_id is not unique.\"\n",
    "\n",
    "        if c.params.use_feature:\n",
    "            store.update(test_df.values)\n",
    "\n",
    "            pred_df = make_feature(\n",
    "                test_df,\n",
    "                store,\n",
    "                feature_set,\n",
    "                load_from_store=False,\n",
    "                save_to_store=False,\n",
    "                debug=c.settings.debug,\n",
    "            )\n",
    "            # assert len(test_df) == len(pred_df), \"test_df and pred_df do not same size.\"\n",
    "            # assert list(pred_df.columns) == feature_cols, \"pred_df has feature_cols columns.\"\n",
    "            # assert np.array_equal(\n",
    "            #     test_df[feature_cols].astype(\"float32\").values, pred_df.values\n",
    "            # ), \"Default features do not match between test_df and pred_df.\"\n",
    "\n",
    "            if c.params.pretrained:\n",
    "                c.params.n_class = c.params.pretrained[0].n_class\n",
    "                preds = inference(c, pred_df, device, models)\n",
    "                predictions_.append(preds)\n",
    "\n",
    "            if c.params.pretrained_2:\n",
    "                pred_df_2 = make_feature(\n",
    "                    test_df,\n",
    "                    store,\n",
    "                    feature_set_2,\n",
    "                    load_from_store=False,\n",
    "                    save_to_store=False,\n",
    "                    debug=c.settings.debug,\n",
    "                )\n",
    "\n",
    "                c.params.n_class = c.params.pretrained_2[0].n_class\n",
    "                preds = inference(c, pred_df_2, device, models_2)\n",
    "                predictions_.append(preds)\n",
    "\n",
    "            if c.params.pretrained_lgb:\n",
    "                preds_lgb = inference_lightgbm(pred_df, models_lgb)\n",
    "                predictions_.append(preds_lgb)\n",
    "\n",
    "        else:\n",
    "            if c.params.pretrained:\n",
    "                preds_ = inference(c, test_df, device, models)\n",
    "                predictions_.append(preds_)\n",
    "\n",
    "            if c.params.pretrained_lgb:\n",
    "                preds_lgb_ = inference_lightgbm(test_df, models_lgb)\n",
    "                predictions_.append(preds_lgb_)\n",
    "\n",
    "        # assert np.array_equal(preds_, preds), \"Predictions do not match between test_df and pred_df.\"\n",
    "        # assert np.array_equal(preds_lgb_, preds_lgb), \"LightGBM predictions do not match between test_df and pred_df.\"\n",
    "\n",
    "        predictions = np.hstack(predictions_)\n",
    "        # predictions = np.hstack([preds_, preds_lgb_])\n",
    "        log.info(f\"predictions size: {predictions.shape}\")\n",
    "\n",
    "        sample_prediction_df[\"target\"] = np.nanmean(predictions, axis=1)\n",
    "\n",
    "        # sample_prediction_df.fillna({\"target\": 0}, inplace=True)\n",
    "\n",
    "        # if c.params.use_feature:\n",
    "        #     store.update_post(sample_prediction_df[[\"row_id\", \"target\"]].values)\n",
    "\n",
    "    except Exception as e:\n",
    "        log.warning(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    env.predict(sample_prediction_df)  # register your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-all-in-one",
   "language": "python",
   "name": "py37-all-in-one"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
