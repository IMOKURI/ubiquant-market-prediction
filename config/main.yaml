defaults:
  # - override hydra/sweeper: optuna
  - _self_

hydra:
  run:
    dir: ../outputs/${now:%Y-%m-%d_%H-%M-%S}
  job_logging:
    formatters:
      simple:
        format: '[%(levelname)s][%(name)s] - %(message)s'
  # sweep:
  #   dir: ../multirun/${now:%Y-%m-%d_%H-%M-%S}
  #   subdir: ${hydra.job.num}
  # sweeper:
  #   study_name: tuning
  #   direction: minimize
  #   n_jobs: 1
  #   n_trials: 10
  #   search_space:
  #     params.seed:
  #       type: int # float
  #       low: 1
  #       high: 1000
  #       # step: 10
  #       # log: True
  #     params.n_fold:
  #       type: categorical
  #       choices: [5, 10]

wandb:
  enabled: True
  entity: imokuri
  project: ump
  dir: ${hydra:runtime.cwd}/../cache

settings:
  print_freq: 100
  gpus: "6,7"

  dirs:
    working: ${hydra:runtime.cwd}
    input: ${hydra:runtime.cwd}/../input/
    # train_image: ${settings.dirs.input}train/
    # test_image: ${settings.dirs.input}test/

  inputs:
    - train.csv
    - example_test.csv
    - example_sample_submission.csv

  debug: False
  n_debug_data: 1000

  amp: True
  multi_gpu: True

params:
  seed: 440
  n_class: 1
  n_fold: 7
  skip_training: false
  epoch: 30
  es_patience: 0
  batch_size: 96
  gradient_acc_step: 1
  max_grad_norm: 1000
  fold: group
  group_name: investment_id
  label_name: target
  dataset: ump_1
  model: ump_1
  # model_name: swin_large_patch4_window12_384_in22k
  pretrained: []
  # pretrained:
  #   - dir: ""
  #     model: ""
  #     name: ""
  dropout: 0.2
  criterion: MSELoss
  optimizer: Adam
  scheduler: CosineAnnealingWarmupRestarts
  lr: 1e-3
  min_lr: 1e-6
  weight_decay: 1e-5
  label_smoothing: 1e-6
  scoring: pearson
